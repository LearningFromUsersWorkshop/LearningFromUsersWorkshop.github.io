([2018 workshop page can be found here](workshop2018.md))

## 2019 Workshop Summary

**Date and Location:** October 20, 2019, 2:20-5:40pm PDT, in Room 3 at [IEEE VIS](http://ieeevis.org/year/2019/welcome)

The Machine Learning from User Interactions (MLUI) workshop seeks to bring together researchers to share their knowledge and build collaborations at the intersection of the Machine Learning and Visualization fields, with a focus on learning from user interaction.  Rather than focusing on what visualization can do to support machine learning (as in current Explainable AI research), this workshop seeks contributions on **how machine learning can support visualization**.  Such support incorporates human-centric sensemaking processes, user-driven analytical systems, and gaining insight from data.  Our intention in this workshop is to generate open discussion about how we currently learn from user interaction, how to build intelligent visualization systems, and how to proceed with future research in this area. We hope to foster discussion regarding systems, interaction models, and interaction techniques. Further, we hope to extend last year's collaborative creation of a research agenda that explores the future of machine learning with user interaction.


# Schedule
**Session 1 (2:20pm - 3:50pm)**
- 2:20pm:  **Introduction and Welcome**
- 2:30pm:  **Keynote:  Mixed-Initiative Visual Analytics: Model-Driven Views and Analytic Guidance** by [Dr. Chris Collins, Ontario Tech University](http://vialab.science.uoit.ca/portfolio/christopher-m-collins)
- 3:00pm:  **Using Machine Learning and Visualization for Qualitative Inductive Analyses of Big Data** by Harshini Priya Muthukrishnan, Danielle Albers Szafir
- 3:10pm:  **"All Right, Mr. DeMille, I'm Ready for My Closeup": Adding Meaning to User Actions from Video for Immersive Analytics** by Andrea Batch, Niklas Elmqvist
- 3:20pm:  **DeepVA: Bridging Cognition and Computation through Semantic Interaction and Deep Learning** by Yali Bian, John Wenskovitch, Chris North
- 3:30pm:  **Shall we play? Extending the Visual Analytics Design Space through Gameful Design Concepts** by Rita Sevastjanova, Hanna Schäfer, Jürgen Bernard, Daniel Keim, Mennatallah El-Assady
- 3:40pm:  **Machine Learning from User Interaction for Visualization and Analytics:  A Workshop-Generated Research Agenda** by John Wenskovitch, Michelle Dowling, Laura Grose, Remco Chang, Alex Endert, David H. Rogers
- 3:50pm:  **Break**

**Session 2 (4:10pm - 5:40pm)**
- 4:10pm:  **Keynote:  Supporting Analytical Conversations** by [Dr. Melanie Tory, Tableau Research](https://research.tableau.com/user/melanie-tory)
- 4:40pm:  **Lightning Talks & Discussion** -- [Submit a Talk](https://forms.gle/xZCnXVmh7spLkGJSA)
- 5:25pm:  **Next Steps**
- 5:40pm:  **Workshop Concludes** ...but go have dinner with each other and keep talking :)



## KEYNOTES

**Keynote 1:  Mixed-Initiative Visual Analytics: Model-Driven Views and Analytic Guidance**

**Abstract:**  The classic information visualization design pattern of "overview first, zoom and filter, details on demand" is no longer sufficient. As datasets have grown (and small displays have become more prevalent), simple overviews tend to be either too high-level to be useful, or too cluttered to reveal anything interesting. The task of exploring and sifting through the data is left only to the analyst in this traditional model. It doesn’t have to be this way: well-designed computational models of various sorts can augment the analysis process by highlighting patterns, suggesting next steps, and drawing attention to regions of potential interest. I seek to facilitate a closely coupled interaction between people and underlying computational models, mediated by visualizations, which include algorithmic transparency as to what guidance is being provided or what data is being hidden. In this talk, I will discuss the role of guidance in user interaction for visualization, framed by several recent research projects which use various types of models to improve the human-computer visual analytic complex.  

**Biography:**  Christopher Collins is the Canada Research Chair in Linguistic Information Visualization and an Associate Professor of Computer Science at Ontario Tech University.  His research focus is interdisciplinary, combining information visualization and human-computer interaction with natural language processing, with a focus on interaction design and guidance in visual analytics.  While his group is best known for text visualization, he collaborates across a wide variety of topics, including health informatics, machine learning, and computer security. His papers have been published in many venues, including IEEE Transactions on Visualization and Computer Graphics, have received awards at IEEE VIS and ACM CHI, and have been featured in popular media such as the CBS News and the New York Times Magazine.  Dr. Collins is a past member of the executive of the IEEE Visualization and Graphics Technical Committee and regularly serves on the IEEE VIS Conference Organizing Committee. He received his Ph.D. in Computer Science from the University of Toronto.



**Keynote 2:  Keynote:  Supporting Analytical Conversations**

**Abstract:**  to come

**Biography:**  to come



## WORKSHOP TOPICS

The topic of the workshop will focus on issues and opportunities related to the use of machine learning to learn from user interaction in the course of data visualization and analysis. Specifically, we will focus on research questions including:

- How are machine learning algorithms currently learning from user interaction, and what other possibilities exist?
- What kinds of interactions can provide feedback to machine learning algorithms?
- What can machine learning algorithms learn from interactions?
- Which machine learning algorithms are most applicable in this domain?
- How can machine learning algorithms be designed to enable user interaction and feedback?
- How can visualizations and interactions be designed to exploit machine learning algorithms?
- How can visualization system architectures be designed to support machine learning?
- How should we manage conflicts between the user's intent and the data or machine learning algorithm capabilities?
- How can we evaluate systems that incorporate both machine learning algorithms and user interaction together?
- How can machine learning and user interaction together make both computation and user cognition more efficient?
- How can we support the sensemaking process by learning from user interaction?

## SUBMISSIONS

We have two submission tracks: for papers and for posters.

### Papers

We invite research and position papers between 5 and 10 pages in length (NOT including references).  All submissions must be formatted according to the [VGTC conference style template](http://junctionpublishing.org/vgtc/Tasks/camera.html) (i.e., NOT the journal style template that full papers use).  Papers are to be submitted online through the [Precision Conference System](https://new.precisionconference.com/submissions/vis19r) **at the MLUI track**.  All papers accepted for presentation at the workshop will be published on IEEE Xplore and linked from the workshop website.  All papers should contain full author names and affiliations.  These papers are considered archival; reuse of the content in a follow-up publication is only permitted in a proper journal, and any extended version must extend the original paper by at least 30%.  If applicable, a link to a short video (up to 5 min. in length) may also be submitted. The papers will be juried by the organizers and selected external reviewers and will be chosen according to relevance, quality, and likelihood that they will stimulate and contribute to the discussion. At least one author of each accepted paper needs to register  for the conference (even if only for the workshop). Registration information will be available on the [IEEE VIS website](http://ieeevis.org/year/2018/welcome).
  
#### Important Dates

Submission deadline:  July 15, 2019 

Author notification:  August 13, 2019 

Camera-ready deadline:  August 22, 2019

Speaker Schedule Available:  September 15, 2019

Workshop:  October 20 or 21, 2019

### Posters

We invite both late-breaking work and contributions in this area from other research domains to submit extended abstracts between 2 and 4 pages in length (NOT including references).  All submissions must be formatted according to the [VGTC conference style template](http://junctionpublishing.org/vgtc/Tasks/camera.html) (i.e., NOT the journal style template that full papers use).  Extended abstracts are to be submitted via email to our GMail account:  [learningfromusersworkshop@gmail.com](mailto:learningfromusersworkshop@gmail.com).  All abstracts accepted for presentation at the workshop will be linked from the workshop website.  All abstracts should contain full author names and affiliations.  If applicable, a link to a short video (up to 5 min. in length) may also be submitted. The abstracts will be juried by the organizers and selected external reviewers and will be chosen according to relevance, quality, and likelihood that they will stimulate and contribute to the discussion. At least one author of each accepted poster needs to register  for the conference (even if only for the workshop). Registration information will be available on the [IEEE VIS website](http://ieeevis.org/year/2018/welcome).
  
#### Important Dates

Submission deadline:  August 20, 2019

Author notification:  September 1, 2019

Camera-ready deadline:  October 1, 2019

Workshop:  October 20 or 21, 2019

## ORGANIZERS

John Wenskovitch, Virginia Tech (jw87@vt.edu)

Michelle Dowling, Virginia Tech (dowlingm@vt.edu)

Chris North, Virginia Tech

Remco Chang, Tufts University

Alex Endert, Georgia Tech

David Rogers, Los Alamos National Lab

Fabian C. Peña, Universidad de los Andes

Sriram Yarlagadda, DePaul University

Eli T. Brown, DePaul University
